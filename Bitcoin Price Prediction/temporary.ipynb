{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8568ff53-1a5c-4b0b-9cd8-61fb6076f756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c802ff4-b192-424b-8b09-1f76c9c7e7ad",
   "metadata": {},
   "source": [
    "\n",
    "- how to train/test split\n",
    "- windowinf\n",
    "- preidctions\n",
    "- evaluatiopn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b44f0f7-3bea-4e84-a308-0edce6db6ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22adcf60-a3c2-4ae5-a4c9-2ae8bab2e770",
   "metadata": {},
   "source": [
    "### Windowing dataset\n",
    "\n",
    "\n",
    "We've got to window our time series.\n",
    "\n",
    "Why do we window?\n",
    "\n",
    "Windowing is a method to turn a time series dataset into a **supervised learning problem**. \n",
    "\n",
    "In other words, we want to use windows of the past to predict the future.\n",
    "\n",
    "For example for a univariate time series, windowing for one week (`window=7`) to predict the next single value (`horizon=1`) might look like:\n",
    "\n",
    "```\n",
    "Window for one week (univariate time series)\n",
    "\n",
    "[0, 1, 2, 3, 4, 5, 6] -> [7]\n",
    "[1, 2, 3, 4, 5, 6, 7] -> [8]\n",
    "[2, 3, 4, 5, 6, 7, 8] -> [9]\n",
    "```\n",
    "\n",
    "Or for the price of Bitcoin, it'd look like:\n",
    "\n",
    "```\n",
    "Window for one week with the target of predicting the next day (Bitcoin prices)\n",
    "\n",
    "[123.654, 125.455, 108.584, 118.674, 121.338, 120.655, 121.795] -> [123.033]\n",
    "[125.455, 108.584, 118.674, 121.338, 120.655, 121.795, 123.033] -> [124.049]\n",
    "[108.584, 118.674, 121.338, 120.655, 121.795, 123.033, 124.049] -> [125.961]\n",
    "```\n",
    "\n",
    "Let's build some functions which take in a univariate time series and turn it into windows and horizons of specified sizes.\n",
    "\n",
    "Now we'll write a function to take in an array and turn it into a window and horizon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0197a34-b79c-4f14-a5b4-63886396759f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to label windowed data\n",
    "def get_labelled_windows(x, horizon=1):\n",
    "  \"\"\"\n",
    "  Creates labels for windowed dataset.\n",
    "\n",
    "  E.g. if horizon=1 (default)\n",
    "  Input: [1, 2, 3, 4, 5, 6] -> Output: ([1, 2, 3, 4, 5], [6])\n",
    "  \"\"\"\n",
    "  return x[:, :-horizon], x[:, -horizon:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a333e52-179c-42f0-9fe9-780fc423b538",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test out the window labelling function with window_size=7 and horizon=1\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m test_window, test_label \u001b[38;5;241m=\u001b[39m get_labelled_windows(\u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mexpand_dims(tf\u001b[38;5;241m.\u001b[39mrange(\u001b[38;5;241m8\u001b[39m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), horizon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWindow: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtf\u001b[38;5;241m.\u001b[39msqueeze(test_window)\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> Label: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtf\u001b[38;5;241m.\u001b[39msqueeze(test_label)\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# Test out the window labelling function with window_size=7 and horizon=1\n",
    "test_window, test_label = get_labelled_windows(tf.expand_dims(tf.range(8)+1, axis=0), horizon=1)\n",
    "print(f\"Window: {tf.squeeze(test_window).numpy()} -> Label: {tf.squeeze(test_label).numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052bc8d7-6c02-45f1-9a82-b2a2700fa6eb",
   "metadata": {},
   "source": [
    "Now we need a way to make windows for an entire time series.\n",
    "\n",
    "We could do this with Python for loops, however, for large time series, that'd be quite slow.\n",
    "\n",
    "To speed things up, we'll leverage [NumPy's array indexing](https://numpy.org/doc/stable/reference/arrays.indexing.html).\n",
    "\n",
    "Let's write a function which:\n",
    "1. Creates a window step of specific window size, for example: `[[0, 1, 2, 3, 4, 5, 6, 7]]`\n",
    "2. Uses NumPy indexing to create a 2D of multiple window steps, for example: \n",
    "```\n",
    "[[0, 1, 2, 3, 4, 5, 6, 7],\n",
    " [1, 2, 3, 4, 5, 6, 7, 8],\n",
    " [2, 3, 4, 5, 6, 7, 8, 9]]\n",
    "```\n",
    "3. Uses the 2D array of multuple window steps to index on a target series\n",
    "4. Uses the `get_labelled_windows()` function we created above to turn the window steps into windows with a specified horizon\n",
    "\n",
    "> 📖 **Resource:** The function created below has been adapted from Syafiq Kamarul Azman's article [*Fast and Robust Sliding Window Vectorization with NumPy*](https://towardsdatascience.com/fast-and-robust-sliding-window-vectorization-with-numpy-3ad950ed62f5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f01de19-2564-4619-b46d-ef0132e34259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to view NumPy arrays as windows \n",
    "def make_windows(x, window_size=7, horizon=1):\n",
    "    \"\"\"\n",
    "    Turns a 1D array into a 2D array of sequential windows of window_size.\n",
    "    \"\"\"\n",
    "    # 1. Create a window of specific window_size (add the horizon on the end for later labelling)\n",
    "    window_step = np.expand_dims(np.arange(window_size+horizon), axis=0)\n",
    "    # print(f\"Window step:\\n {window_step}\")\n",
    "\n",
    "    # 2. Create a 2D array of multiple window steps (minus 1 to account for 0 indexing)\n",
    "    window_indexes = window_step + np.expand_dims(np.arange(len(x)-(window_size+horizon-1)), axis=0).T # create 2D array of windows of size window_size\n",
    "    # print(f\"Window indexes:\\n {window_indexes[:3], window_indexes[-3:], window_indexes.shape}\")\n",
    "\n",
    "    # 3. Index on the target array (time series) with 2D array of multiple window steps\n",
    "    windowed_array = x[window_indexes]\n",
    "\n",
    "    # 4. Get the labelled windows\n",
    "    windows, labels = get_labelled_windows(windowed_array, horizon=horizon)\n",
    "\n",
    "    return windows, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8338be19-5dd8-4299-87ec-e353023eba18",
   "metadata": {},
   "source": [
    "> 🔑 **Note:** You can find a function which achieves similar results to the ones we implemented above at [`tf.keras.preprocessing.timeseries_dataset_from_array()`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/timeseries_dataset_from_array). Just like ours, it takes in an array and returns a windowed dataset. It has the benefit of returning data in the form of a tf.data.Dataset instance (we'll see how to do this with our own data later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9515ff9f-5c79-47b4-b56a-7e0489a01b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cb1e81-b823-4be0-b2a1-c760bb9244db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96634cbb-b278-464a-8ee0-63bb07848b59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "656350f9-3dc4-4bb0-af6f-4121936ce3b2",
   "metadata": {},
   "source": [
    "## PREDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fa0f354-e370-4685-a709-9ec6744e07ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrong way!!!\n",
    "# This way of making predictions is not correct, but it can be useful for a quick comparison of the models.\n",
    "def make_preds(model, input_data):\n",
    "  \"\"\"\n",
    "  Uses model to make predictions on input_data.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  model: trained model \n",
    "  input_data: windowed input data (same kind of data model was trained on)\n",
    "\n",
    "  Returns model predictions on input_data.\n",
    "  \"\"\"\n",
    "  forecast = model.predict(input_data)\n",
    "  return tf.squeeze(forecast) # return 1D array of predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91df9540-a383-4dc6-9719-8b35ae349ced",
   "metadata": {},
   "source": [
    "------------DIBUJO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e480b620-ca83-459b-a100-328ce05af493",
   "metadata": {},
   "source": [
    "<img src='images/preds.png'/>\n",
    "*Example flow chart representing the loop we're about to create for making forecasts. Not pictured: retraining a forecasting model every time a forecast is made & new data is acquired. For example, if you're predicting the price of Bitcoin daily, you'd want to retrain your model every day, since each day you're going to have a new data point to work with.*\n",
    "\n",
    "Alright, let's create a function which returns `INTO_FUTURE` forecasted values using a trained model.\n",
    "\n",
    "To do so, we'll build the following steps:\n",
    "1. Function which takes as input: \n",
    "  * a list of values (the Bitcoin historical data)\n",
    "  * a trained model (such as `model_9`)\n",
    "  * a window into the future to predict (our `INTO_FUTURE` variable)\n",
    "  * the window size a model was trained on (`WINDOW_SIZE`) - the model can only predict on the same kind of data it was trained on\n",
    "2. Creates an empty list for future forecasts (this will be returned at the end of the function) and extracts the last `WINDOW_SIZE` values from the input values (predictions will start from the last `WINDOW_SIZE` values of the training data)\n",
    "3. Loop `INTO_FUTURE` times making a prediction on `WINDOW_SIZE` datasets which update to remove the first the value and append the latest prediction \n",
    "  * Eventually future predictions will be made using the model's own previous predictions as input.\n",
    "  \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d26b4309-f29b-4778-af5d-cc43f5299933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_future_forecast(model, initial_window, into_future, window_size=32, verbose=False) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate future predictions using a sliding window approach.\n",
    "\n",
    "    Parameters:\n",
    "    - model: A trained TensorFlow/Keras model that takes input of shape (1, window_size) and outputs one prediction.\n",
    "    - initial_window (np.ndarray): The initial time window to start predictions from (length = window_size).\n",
    "    - into_future (int): Number of future time steps to predict.\n",
    "    - window_size (int): Size of the input window for the model. Default is G.WINDOW_SIZE.\n",
    "    - verbose (bool): If True, prints each prediction step.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Array of predicted values with shape (into_future,).\n",
    "    \"\"\"\n",
    "    future_forecast = []\n",
    "    last_window = initial_window\n",
    "\n",
    "    for step in range(into_future):\n",
    "        future_pred = model.predict(tf.expand_dims(last_window, axis=0), verbose=0)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Predicting on: \\n {last_window} -> Prediction: {tf.squeeze(future_pred).numpy()}\\n\")\n",
    "\n",
    "        future_forecast.append(tf.squeeze(future_pred).numpy())\n",
    "        last_window = np.append(last_window, future_pred[0][0])[-window_size:]\n",
    "\n",
    "    return np.array(future_forecast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "785265a9-e914-4b88-b322-5b951071062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for multi-step forecasting (horizon > 1)\n",
    "\n",
    "def make_future_forecast(model, initial_window, into_future, horizon, window_size=32, verbose=False) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate future predictions using a sliding window for models that output multiple steps at once.\n",
    "\n",
    "    Parameters:\n",
    "    - model: A trained TensorFlow/Keras model that takes input of shape (1, window_size) and outputs (1, horizon).\n",
    "    - initial_window (np.ndarray): The initial time window to start predictions from (length = window_size).\n",
    "    - into_future (int): Total number of time steps to forecast (not number of prediction rounds).\n",
    "    - horizon (int): Number of steps the model predicts at each call.\n",
    "    - window_size (int): Size of the input window for the model.\n",
    "    - verbose (bool): If True, prints each prediction step.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Array of shape (into_future,) with all predictions concatenated and trimmed to match the exact horizon.\n",
    "    \"\"\"\n",
    "    future_forecast = []\n",
    "    last_window = initial_window.copy()\n",
    "    \n",
    "    steps = int(np.ceil(into_future / horizon))\n",
    "\n",
    "    for _ in range(steps):\n",
    "        future_pred = model.predict(tf.expand_dims(last_window, axis=0), verbose=0)  # shape: (1, horizon)\n",
    "        future_pred = tf.squeeze(future_pred).numpy()  # shape: (horizon,)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Predicting on: \\n{last_window} -> Prediction: {future_pred}\\n\")\n",
    "\n",
    "        future_forecast.extend(future_pred.tolist())\n",
    "\n",
    "        # Slide the window forward by appending the predictions\n",
    "        last_window = np.append(last_window, future_pred)[-window_size:]\n",
    "\n",
    "    return np.array(future_forecast[:into_future])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4045fa99-af06-436c-bd0d-d21692e19bea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe27acf-c3f7-4446-b7ce-ffee548a4b75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e74e6b73-9366-4533-b4e6-8914bc1ca97a",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6b741f2-b93e-40c6-8b51-8b3793303355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MASE implemented courtesy of sktime - https://github.com/alan-turing-institute/sktime/blob/ee7a06843a44f4aaec7582d847e36073a9ab0566/sktime/performance_metrics/forecasting/_functions.py#L16\n",
    "def mean_absolute_scaled_error(y_true, y_pred):\n",
    "  \"\"\"\n",
    "  Implement MASE (assuming no seasonality of data).\n",
    "  \"\"\"\n",
    "  mae = tf.reduce_mean(tf.abs(y_true - y_pred))\n",
    "\n",
    "  # Find MAE of naive forecast (no seasonality)\n",
    "  mae_naive_no_season = tf.reduce_mean(tf.abs(y_true[1:] - y_true[:-1])) # our seasonality is 1 day (hence the shifting of 1 day)\n",
    "\n",
    "  return mae / mae_naive_no_season\n",
    "\n",
    "\n",
    "\n",
    "# single horizon\n",
    "def evaluate_preds(y_true, y_pred):\n",
    "    # Make sure float32 (for metric calculations)\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "\n",
    "    # Calculate various metrics\n",
    "    mae = tf.keras.metrics.mean_absolute_error(y_true, y_pred)\n",
    "    mse = tf.keras.metrics.mean_squared_error(y_true, y_pred) # puts and emphasis on outliers (all errors get squared)\n",
    "    rmse = tf.sqrt(mse)\n",
    "    mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)\n",
    "    mase = mean_absolute_scaled_error(y_true, y_pred)\n",
    "\n",
    "    return {\"mae\": mae.numpy(),\n",
    "          \"mse\": mse.numpy(),\n",
    "          \"rmse\": rmse.numpy(),\n",
    "          \"mape\": mape.numpy(),\n",
    "          \"mase\": mase.numpy()}\n",
    "\n",
    "\n",
    "# multi-step horizon\n",
    "def evaluate_preds_generalized(y_true, y_pred):\n",
    "  # Make sure float32 (for metric calculations)\n",
    "  y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "  y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "\n",
    "  # Calculate various metrics\n",
    "  mae = tf.keras.metrics.mean_absolute_error(y_true, y_pred)\n",
    "  mse = tf.keras.metrics.mean_squared_error(y_true, y_pred)\n",
    "  rmse = tf.sqrt(mse)\n",
    "  mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)\n",
    "  mase = mean_absolute_scaled_error(y_true, y_pred)\n",
    "\n",
    "  # Account for different sized metrics (for longer horizons, reduce to single number)\n",
    "  if mae.ndim > 0: # if mae isn't already a scalar, reduce it to one by aggregating tensors to mean\n",
    "    mae = tf.reduce_mean(mae)\n",
    "    mse = tf.reduce_mean(mse)\n",
    "    rmse = tf.reduce_mean(rmse)\n",
    "    mape = tf.reduce_mean(mape)\n",
    "    mase = tf.reduce_mean(mase)\n",
    "\n",
    "  return {\"mae\": mae.numpy(),\n",
    "          \"mse\": mse.numpy(),\n",
    "          \"rmse\": rmse.numpy(),\n",
    "          \"mape\": mape.numpy(),\n",
    "          \"mase\": mase.numpy()}\n",
    "\n",
    "\n",
    "\n",
    "# better!!!!!!!!!!!!!!!!!1\n",
    "def evaluate_preds(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Evaluate forecast predictions using common regression metrics.\n",
    "    Works with both single-step and multi-step predictions. Automatically\n",
    "    reduces multi-step metrics to scalars by averaging.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true (array-like): Ground truth values (1D or 2D).\n",
    "    - y_pred (array-like): Predicted values (same shape as y_true).\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary with MAE, MSE, RMSE, MAPE, and MASE scores.\n",
    "    \"\"\"\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "\n",
    "    mae = tf.reduce_mean(tf.keras.metrics.mean_absolute_error(y_true, y_pred))\n",
    "    mse = tf.reduce_mean(tf.keras.metrics.mean_squared_error(y_true, y_pred))\n",
    "    rmse = tf.sqrt(mse)\n",
    "    mape = tf.reduce_mean(tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred))\n",
    "    mase = tf.reduce_mean(mean_absolute_scaled_error(y_true, y_pred))\n",
    "\n",
    "    return {\n",
    "        \"mae\": mae.numpy(),\n",
    "        \"mse\": mse.numpy(),\n",
    "        \"rmse\": rmse.numpy(),\n",
    "        \"mape\": mape.numpy(),\n",
    "        \"mase\": mase.numpy()\n",
    "    }\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ac7313-27c4-4fc0-a7d1-3b64bef5e6ad",
   "metadata": {},
   "source": [
    "Yes, your function is appropriate for evaluating multi-step forecasts (i.e., horizon > 1), as long as:\n",
    "\n",
    "y_true and y_pred have the same shape, e.g., (num_samples, horizon) or just (horizon,).\n",
    "\n",
    "The model outputs multiple steps at once (direct multi-step), and you're evaluating the whole sequence against the ground truth.\n",
    "\n",
    "✅ Why It Works for horizon > 1\n",
    "TensorFlow's built-in metrics like mean_absolute_error return a vector when comparing (batch_size, horizon) arrays — one MAE per time step.\n",
    "\n",
    "You correctly reduce them with tf.reduce_mean(...) to get a single scalar per metric.\n",
    "\n",
    "Your fallback to .numpy() is fine for returning usable values in Python.\n",
    "\n",
    "✅ Good Design Choices\n",
    "✅ Type casting to float32\n",
    "\n",
    "✅ Handles both scalar and vector metrics\n",
    "\n",
    "✅ General enough for both single- and multi-step evaluation\n",
    "\n",
    "✅ Custom mase function is included — good for time series\n",
    "\n",
    "⚠️ Minor Suggestions\n",
    "If your data is 1D (no batch dimension), e.g. y_true.shape == (horizon,), and you're comparing it with y_pred.shape == (horizon,), it still works — but you could clarify this in the docstring (if added later).\n",
    "\n",
    "If your mean_absolute_scaled_error() function expects sequences too, make sure it's compatible with multi-step input.\n",
    "\n",
    "Let me know if you want to:\n",
    "\n",
    "Add support for per-time-step metric output (e.g., error at step 1, step 2, etc.)\n",
    "\n",
    "Add a docstring\n",
    "\n",
    "Support batched predictions (e.g., shape = (N, horizon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891decc1-82ee-4567-92f1-a2744c05406a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118d3da5-4c0f-4dfb-8366-9ec047690fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bcf2ad-d7af-453a-b586-e84e3fc418dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acd9b53-c313-4275-9373-0bbbeace212e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ac944d-696a-4d86-8ce7-1e547fbdfcd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d29e438-96d4-4354-b87e-4bae1ae573f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a1f7c7-1344-4a40-88bd-70c8bbb6313d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "353cfa5b-21dd-4c7d-aae7-9ee5472dbf39",
   "metadata": {},
   "source": [
    "✅ Why the Test Set is a “Pseudofuture”\n",
    "In time series, you simulate the forecasting task by splitting historical data into:\n",
    "\n",
    "Training set: Past data used to train the model.\n",
    "\n",
    "Test set: Later part of the historical data, treated as if it's the future.\n",
    "\n",
    "This test set lets you evaluate how your model might behave on future unseen data, but:\n",
    "\n",
    "It's not the actual future — the data already exists.\n",
    "\n",
    "Your model may perform differently when deployed, especially if real-world conditions change (called data drift or concept drift).\n",
    "\n",
    "\n",
    "> 📖 **Resource:** Working with time series data can be tricky compared to other kinds of data. And there are a few pitfalls to watch out for, such as how much data to use for a test set. The article [*3 facts about time series forecasting that surprise experienced machine learning practitioners*](https://towardsdatascience.com/3-facts-about-time-series-forecasting-that-surprise-experienced-machine-learning-practitioners-69c18ee89387) talks about different things to watch out for when working with time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93978fd6-0219-4dda-ac40-057c630497de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d2a952b-45e5-4a01-8db8-a020caa22deb",
   "metadata": {},
   "source": [
    "Before we discuss what modelling experiments we're going to run, there are two terms you should be familiar with, **horizon** and **window**. \n",
    "  * **horizon** = number of timesteps to predict into future\n",
    "  * **window** = number of timesteps from past used to predict **horizon**\n",
    "\n",
    "For example, if we wanted to predict the price of Bitcoin for tomorrow (1 day in the future) using the previous week's worth of Bitcoin prices (7 days in the past), the horizon would be 1 and the window would be 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ae5b89-79a7-472a-a2ae-0125c61211a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96336c0e-8d5f-4430-858e-3ead0466adc6",
   "metadata": {},
   "source": [
    "One of the most common baseline models for time series forecasting, the naïve model (also called the [naïve forecast](https://otexts.com/fpp3/simple-methods.html#na%C3%AFve-method)), requires no training at all.\n",
    "\n",
    "That's because all the naïve model does is use the previous timestep value to predict the next timestep value.\n",
    "\n",
    "The formula looks like this:\n",
    "\n",
    "$$\\hat{y}_{t} = y_{t-1}$$ \n",
    "\n",
    "In English: \n",
    "> The prediction at timestep `t` (y-hat) is equal to the value at timestep `t-1` (the previous timestep).\n",
    "\n",
    "\n",
    "In an open system (like a stock market or crypto market), you'll often find beating the naïve forecast with *any* kind of model is quite hard.\n",
    "\n",
    "> 🔑 **Note:** For the sake of this notebook, an **open system** is a system where inputs and outputs can freely flow, such as a market (stock or crypto). Where as, a **closed system** the inputs and outputs are contained within the system (like a poker game with your buddies, you know the buy in and you know how much the winner can get). Time series forecasting in **open systems** is generally quite poor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0355b17c-b093-498f-a9ca-6ddd3c2affb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bf9c65-5a71-41d6-88d9-2ad04b47f6dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f74d43b-43d9-40dc-9965-03b4b396b9ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f986e4fb-6fe7-4500-b8f9-22fc0e559c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98eea083-4807-4677-8d16-b689b863cf0d",
   "metadata": {},
   "source": [
    "ANALIZAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a32936a0-dfc6-4d10-b9f4-46077e3d9dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_future_forecast_generalized(model, initial_window, into_future, horizon, window_size=32, verbose=False) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate future predictions using a sliding window for models that output multiple steps at once.\n",
    "    Returns predictions as a 2D array of shape (num_forecasts, horizon).\n",
    "    \"\"\"\n",
    "    future_forecast = []\n",
    "    last_window = initial_window.copy()\n",
    "    \n",
    "    steps = int(np.ceil(into_future / horizon))\n",
    "\n",
    "    for _ in range(steps):\n",
    "        future_pred = model.predict(tf.expand_dims(last_window, axis=0), verbose=0)  # shape: (1, horizon)\n",
    "        future_pred = tf.squeeze(future_pred).numpy()  # shape: (horizon,)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Predicting on: \\n{last_window} -> Prediction: {future_pred}\\n\")\n",
    "\n",
    "        future_forecast.append(future_pred)  # Append as row, not flattened\n",
    "\n",
    "        # Update window\n",
    "        last_window = np.append(last_window, future_pred)[-window_size:]\n",
    "\n",
    "    # Return as (num_steps, horizon), trim excess if needed\n",
    "    forecast_array = np.array(future_forecast)  # shape: (steps, horizon)\n",
    "    \n",
    "    # If exact match is needed with test_labels shape:\n",
    "    total_needed = (into_future // horizon)\n",
    "    return forecast_array[:total_needed]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30824a6f-3163-4334-8eff-4618a3f09d51",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#into_future = len(test_labels) * horizon\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m future_forecast_3 \u001b[38;5;241m=\u001b[39m make_future_forecast_generalized(model\u001b[38;5;241m=\u001b[39m\u001b[43mmodel_3\u001b[49m,\n\u001b[1;32m      3\u001b[0m                                        initial_window\u001b[38;5;241m=\u001b[39mtest_windows[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m      4\u001b[0m                                        into_future\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(test_labels)\u001b[38;5;241m*\u001b[39mHORIZON, \u001b[38;5;66;03m# NEW\u001b[39;00m\n\u001b[1;32m      5\u001b[0m                                        horizon\u001b[38;5;241m=\u001b[39mHORIZON,            \n\u001b[1;32m      6\u001b[0m                                        window_size\u001b[38;5;241m=\u001b[39mWINDOW_SIZE)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# HACER LO DEL HORIZONTE MULTIPLE EN MI BACKGROUND CON DATIS SIMPLES Y LISTO! \u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_3' is not defined"
     ]
    }
   ],
   "source": [
    "#into_future = len(test_labels) * horizon\n",
    "future_forecast_3 = make_future_forecast_generalized(model=model_3,\n",
    "                                       initial_window=test_windows[0],\n",
    "                                       into_future=len(test_labels)*HORIZON, # NEW\n",
    "                                       horizon=HORIZON,            \n",
    "                                       window_size=WINDOW_SIZE)\n",
    "\n",
    "# HACER LO DEL HORIZONTE MULTIPLE EN MI BACKGROUND CON DATIS SIMPLES Y LISTO! \n",
    "                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748e3a8f-9522-4366-95bc-e0e46109220e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
